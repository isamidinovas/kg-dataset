# CORRECT
import os
import time
import re
import json
import google.generativeai as genai
import pandas as pd
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env
load_dotenv()
api_key = os.getenv("GENAI_API_KEY")
if not api_key:
    raise ValueError("API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª .env")

genai.configure(api_key=api_key)
model = genai.GenerativeModel("gemini-2.5-flash")

AUTO_PROMPT = (
    "–ë–µ—Ä–∏–ª–≥–µ–Ω —Ç–µ–∫—Å—Ç—Ç–∏–Ω –Ω–µ–≥–∏–∑–∏–Ω–¥–µ –∫—ã—Ä–≥—ã–∑ —ç–ª–∏–Ω–∏–Ω —Ç–∞—Ä—ã—Ö—ã–Ω–∞, —Ç–∏–ª–∏–Ω–µ, –≥–µ–æ–≥—Ä–∞—Ñ–∏—è—Å—ã–Ω–∞, —ç–∫–æ–Ω–æ–º–∏–∫–∞—Å—ã–Ω–∞, –º–∞–¥–∞–Ω–∏—è—Ç—ã–Ω–∞, —Å–∞–ª—Ç-—Å–∞–Ω–∞–∞—Å—ã–Ω–∞, –¥“Ø–π–Ω”© —Ç–∞–∞–Ω—ã–º—ã–Ω–∞ –∂–∞–Ω–∞ –±–∞—à–∫–∞ —É—à—É–ª —Å—ã—è–∫—Ç—É—É –∞—Å–ø–µ–∫—Ç–∏–ª–µ—Ä–∏–Ω–µ –±–∞–π–ª–∞–Ω—ã—à—Ç—É—É —Ç–µ—Ä–µ“£, –º–∞–∞–ª—ã–º–∞—Ç—Ç—É—É –∂–∞–Ω–∞ —Å–∞–ø–∞—Ç—Ç—É—É –¥–∞—Ç–∞—Å–µ—Ç—Ç–∏ —Ç“Ø–∑, –∞–ª 7 –∂—É–ø ¬´—Å—É—Ä–æ–æ-–∂–æ–æ–ø—Ç–æ–Ω¬ª —Ç—É—Ä—Å—É–Ω.\n\n"
    "–¢–∏–ª–∏:\n"
    "- –ë–∞—Ä–¥—ã–∫ —Å—É—Ä–æ–æ–ª–æ—Ä –∂–∞–Ω–∞ –∂–æ–æ–ø—Ç–æ—Ä –∫—ã—Ä–≥—ã–∑ —Ç–∏–ª–∏–Ω–¥–µ –≥–∞–Ω–∞ –∂–∞–∑—ã–ª—ã—à—ã –∫–µ—Ä–µ–∫.\n\n"
    "–°—É—Ä–æ–æ–ª–æ—Ä–≥–æ –∫–æ—é–ª–≥–∞–Ω —Ç–∞–ª–∞–ø—Ç–∞—Ä:\n"
    "- –ê—Ä –±–∏—Ä —Å—É—Ä–æ–æ –∫–µ–º–∏–Ω–¥–µ 1000 —Å–∏–º–≤–æ–ª–¥–æ–Ω —Ç—É—Ä—É—à—É –∫–µ—Ä–µ–∫.\n"
    "- –°—É—Ä–æ–æ –ª–æ–≥–∏–∫–∞–ª—ã–∫ –∂–∞–∫—Ç–∞–Ω —Ç–æ–ª—É–∫, –∫–µ“£–∏—Ä–∏ –∂–∞–π—ã–ª–≥–∞–Ω, —Ç–µ–º–∞–Ω—ã–Ω 1-2 –≥–∞–Ω–∞ ”©–∑ –∞—Ä–∞ –±–∞–π–ª–∞–Ω—ã—à–∫–∞–Ω –∞—Å–ø–µ–∫—Ç–∏—Å–∏–Ω –∫–∞–º—Ç—ã—à—ã –∫–µ—Ä–µ–∫.\n"
    "- –°—É—Ä–æ–æ–ª–æ—Ä —Ç–∞–∫, —Ç–µ–º–∞—Ç–∏–∫–∞–ª—ã–∫ –∂–∞–∫—Ç–∞–Ω —Ñ–æ–∫—É—Å—Ç–∞–ª–≥–∞–Ω, –∞—Ä –∫–∞–Ω–¥–∞–π —Ç–µ–º–∞–ª–∞—Ä –º–µ–Ω–µ–Ω –∞—à—ã–∫—á–∞ –∂“Ø–∫—Ç”©–ª–±”©–≥”©–Ω –±–æ–ª—É—à—É –∫–µ—Ä–µ–∫.\n"
    "- –ö–µ—Ä–µ–∫ –±–æ–ª—Å–æ, —Ç–∞–∫—Ç–æ–æ—á—É –¥–µ—Ç–∞–ª–¥–∞—Ä–¥—ã –∂–µ –±–∞–π–ª–∞–Ω—ã—à–∫–∞–Ω –ø—É–Ω–∫—Ç—Ç–∞—Ä–¥—ã –∫–æ—à—É—É–≥–∞ –±–æ–ª–æ—Ç, –±–∏—Ä–æ–∫ –∞–ª–∞—Ä–¥—ã–Ω –±–∞—Ä–¥—ã–≥—ã –±–∏—Ä –Ω–µ–≥–∏–∑–≥–∏ —Å—É—Ä–æ–æ–≥–æ —Ç–∏–µ—à–µ–ª“Ø“Ø –±–æ–ª—É—à—É –∫–µ—Ä–µ–∫.\n"
    "- ¬´–¢–µ–∫—Å—Ç–∫–µ —ã–ª–∞–π—ã–∫¬ª, ¬´—Ç–µ–∫—Å—Ç—Ç–µ –±–µ—Ä–∏–ª–≥–µ–Ω–¥–µ–π¬ª, ¬´—Ç–µ–∫—Å—Ç—Ç–µ¬ª, ¬´—Ç–µ–∫—Å—Ç—Ç–µ–≥–∏¬ª, ¬´—Ç–µ–∫—Å—Ç—Ç–µ –∞–π—Ç—ã–ª–≥–∞–Ω–¥–∞–π¬ª, ¬´–∂–æ–≥–æ—Ä—É–¥–∞–≥—ã –º–∞–∞–ª—ã–º–∞—Ç—Ç–∞—Ä–≥–∞ —Ç–∞—è–Ω—ã–ø¬ª —Å—ã—è–∫—Ç—É—É —Å”©–∑ –∞–π–∫–∞—à—Ç–∞—Ä—ã–Ω –∂–∞–Ω–∞ —É—à—É–≥–∞ –æ–∫—à–æ—à —à–∏–ª—Ç–µ–º–µ–ª–µ—Ä–¥–∏ –∫–æ–ª–¥–æ–Ω–±–æ.\n\n"
    "–ñ–æ–æ–ø—Ç–æ—Ä–≥–æ –∫–æ—é–ª–≥–∞–Ω —Ç–∞–ª–∞–ø—Ç–∞—Ä:\n"
    "- –ê—Ä –±–∏—Ä –∂–æ–æ–ø –∫–µ–º–∏–Ω–¥–µ 1000 —Å–∏–º–≤–æ–ª–¥–æ–Ω —Ç—É—Ä—É—à—É –∫–µ—Ä–µ–∫.\n"
    "- –ñ–æ–æ–ø—Ç–æ—Ä –ª–æ–≥–∏–∫–∞–ª—ã–∫, —Ñ–∞–∫—Ç—ã–ª–∞—Ä–≥–∞ –Ω–µ–≥–∏–∑–¥–µ–ª–≥–µ–Ω, –º–∏—Å–∞–ª–¥–∞—Ä, —Ç–∞—Ä—ã—Ö—ã–π –∂–∞–Ω–∞ —ç—Ç–Ω–æ–≥—Ä–∞—Ñ–∏—è–ª—ã–∫ –¥–µ—Ç–∞–ª–¥–∞—Ä –º–µ–Ω–µ–Ω –±–µ—Ä–∏–ª–∏—à–∏ –∫–µ—Ä–µ–∫.\n"
    "- –ñ–æ–æ–ø –±–µ—Ä–∏–ª–≥–µ–Ω —Å—É—Ä–æ–æ–Ω—É —Ç–æ–ª—É–∫ –∂–∞–Ω–∞ —Ç–µ—Ä–µ“£ –∞—á—ã–ø –±–µ—Ä–∏—à–∏ –∫–µ—Ä–µ–∫.\n\n"
    "- ¬´–¢–µ–∫—Å—Ç–∫–µ —ã–ª–∞–π—ã–∫¬ª, ¬´—Ç–µ–∫—Å—Ç—Ç–µ –±–µ—Ä–∏–ª–≥–µ–Ω–¥–µ–π¬ª, ¬´—Ç–µ–∫—Å—Ç—Ç–µ¬ª, ¬´—Ç–µ–∫—Å—Ç—Ç–µ–≥–∏¬ª, ¬´—Ç–µ–∫—Å—Ç—Ç–µ –∞–π—Ç—ã–ª–≥–∞–Ω–¥–∞–π¬ª, ¬´–∂–æ–≥–æ—Ä—É–¥–∞–≥—ã –º–∞–∞–ª—ã–º–∞—Ç—Ç–∞—Ä–≥–∞ —Ç–∞—è–Ω—ã–ø¬ª —Å—ã—è–∫—Ç—É—É —Å”©–∑ –∞–π–∫–∞—à—Ç–∞—Ä—ã–Ω –∂–∞–Ω–∞ —É—à—É–≥–∞ –æ–∫—à–æ—à —à–∏–ª—Ç–µ–º–µ–ª–µ—Ä–¥–∏ –∫–æ–ª–¥–æ–Ω–±–æ.\n\n"
    "–§–æ—Ä–º–∞—Ç: –≠—á –∫–∞–Ω–¥–∞–π –∫–æ—à—É–º—á–∞ —Ç“Ø—à“Ø–Ω–¥“Ø—Ä–º”©—Å“Ø –∂–æ–∫ JSON-–º–∞—Å—Å–∏–≤–¥–∏ –≥–∞–Ω–∞ –∫–∞–π—Ç–∞—Ä. –ú–∏—Å–∞–ª—ã:\n"
    "[\n  {¬´question¬´: ¬´...¬ª, ¬´answer¬ª:¬´...¬ª},\n  ...\n]"
)


def split_text_by_paragraphs(text: str, paragraphs_per_chunk: int = 50) -> list[str]:
    paragraphs = [p.strip() for p in text.split("\n") if p.strip()]
    return ["\n".join(paragraphs[i:i + paragraphs_per_chunk])
            for i in range(0, len(paragraphs), paragraphs_per_chunk)]


def clean_response(text: str) -> str:
    return re.sub(r"```(?:json)?|```", "", text).strip()


def extract_qa_pairs(text: str) -> list[tuple[str, str]]:
    text = clean_response(text)
    try:
        data = json.loads(text)
        return [(item['question'], item['answer']) for item in data if 'question' in item and 'answer' in item]
    except json.JSONDecodeError:
        print("‚ö†Ô∏è JSON –Ω–µ–≤–∞–ª–∏–¥–µ–Ω, –ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –≤—Ä—É—á–Ω—É—é —á–µ—Ä–µ–∑ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è...")

    pattern = re.compile(
        r'{"question"\s*:\s*"(?P<question>.*?)"\s*,\s*"answer"\s*:\s*"(?P<answer>.*?)"}',
        re.DOTALL
    )
    matches = pattern.findall(text)
    return matches

def main():
    file_path = "data/–ê–¥–∞–±–∏—è—Ç —Ç–µ–æ—Ä–∏—è—Å—ã(okuma.kg)_–ö—ã—Ä–≥—ã–∑ —Ç–∏–ª–∏ –∂–∞–Ω–∞ –∞–¥–∞–±–∏—è—Ç—ã71.txt"
    output_file = "–ê–¥–∞–±–∏—è—Ç —Ç–µ–æ—Ä–∏—è—Å—ã(okuma.kg)_–ö—ã—Ä–≥—ã–∑ —Ç–∏–ª–∏ –∂–∞–Ω–∞ –∞–¥–∞–±–∏—è—Ç—ã71.xlsx"

    if not os.path.exists(file_path):
        print(f"‚ùå –§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return

    with open(file_path, "r", encoding="utf-8") as f:
        full_text = f.read()
    print(f"üìè –û–±—â–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(full_text):,} —Å–∏–º–≤–æ–ª–æ–≤")

    chunks = split_text_by_paragraphs(full_text, paragraphs_per_chunk=50)
    print(f"‚úÖ –†–∞–∑–±–∏—Ç–æ –Ω–∞ {len(chunks)} —á–∞—Å—Ç–µ–π –ø–æ 50 –∞–±–∑–∞—Ü–µ–≤")

    # –ó–∞–≥—Ä—É–∑–∫–∞ —É–∂–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    if os.path.exists(output_file):
        df = pd.read_excel(output_file)
        seen_questions = set(df["–í–æ–ø—Ä–æ—Å"].tolist())
        processed_count = len(df)
        data = df.to_dict("records")
        print(f"üîÅ –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å —á–∞—Å—Ç–∏ {processed_count + 1}")
    else:
        seen_questions = set()
        processed_count = 0
        data = []

    for idx, chunk in enumerate(chunks):
        if idx < processed_count:
            print(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫ —á–∞—Å—Ç–∏ {idx + 1} (—É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞)")
            continue

        print(f"\nüîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Å—Ç–∏ {idx + 1} –∏–∑ {len(chunks)}...")
        full_prompt = f"{AUTO_PROMPT}\n\n–¢–µ–∫—Å—Ç:\n{chunk}"

        try:
            response = model.generate_content(full_prompt)
            cleaned = clean_response(response.text)
            qa_pairs = extract_qa_pairs(cleaned)

            if qa_pairs:
                new_rows = []
                for q, a in qa_pairs:
                    q = q.strip()
                    a = a.strip()
                    if q in seen_questions:
                        continue
                    seen_questions.add(q)
                    new_rows.append({
                        "–í–æ–ø—Ä–æ—Å": q,
                        "–û—Ç–≤–µ—Ç": a,
                        "–î–ª–∏–Ω–∞ –≤–æ–ø—Ä–æ—Å–∞": len(q),
                        "–î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞": len(a)
                    })
                data.extend(new_rows)

                # üíæ –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —á–∞—Å—Ç–∏
                pd.DataFrame(data).to_excel(output_file, index=False)
                print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(new_rows)} –Ω–æ–≤—ã—Ö –ø–∞—Ä, —Ñ–∞–π–ª –æ–±–Ω–æ–≤–ª—ë–Ω.")
            else:
                print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ø–∞—Ä—ã –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            with open("errors.txt", "a", encoding="utf-8") as log:
                log.write(f"\n--- –ß–∞—Å—Ç—å {idx + 1} ---\n{chunk}\n–û—à–∏–±–∫–∞: {e}\n")

        time.sleep(30)

    print(f"\nüìÅ –ì–æ—Ç–æ–≤–æ. –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_file}")


if __name__ == "__main__":
    main()